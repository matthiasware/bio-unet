{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, merge, UpSampling2D, Cropping2D, ZeroPadding2D, Reshape, core, Convolution2D, Conv2DTranspose, Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from model_03 import get_model\n",
    "import pickle\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import load_model\n",
    "import losses\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = utils.get_files(\"data/split\")\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(files)\n",
    "#files = files[0:10]\n",
    "n_valid = int(np.ceil(len(files) * 0.15))\n",
    "n_test = int(np.ceil((len(files) - n_valid)* 0.15))\n",
    "n_train = len(files) - n_valid - n_test\n",
    "f_train = files[0:n_train]\n",
    "f_valid = files[n_train: n_train + n_valid]\n",
    "f_test = files[n_train + n_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(f_train))\n",
    "print(len(f_test))\n",
    "print(len(f_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = utils.load_data(f_train)\n",
    "X_valid, Y_valid = utils.load_data(f_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(img_size=512, n_kernels=3)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.make_bce_loss(0.9971, 0.0029),\n",
    "              optimizer=optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.98, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "#K.get_session().run(tf.global_variables_initializer())\n",
    "#tensorboard = TensorBoard(log_dir=\"logs\") \n",
    "callbacks = [\n",
    "             EarlyStopping(monitor='val_loss',\n",
    "                           patience=4,\n",
    "                           verbose=1),\n",
    "             ModelCheckpoint(\"model07.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size determines the number of samples in each mini batch\n",
    "# steps_per_epoch the number of batch iterations before a training epoch is considered finished.\n",
    "# validation_steps\n",
    "hist = model.fit(x=X_train,\n",
    "                 y=Y_train,\n",
    "                 epochs=10,\n",
    "                 verbose=2, callbacks=callbacks,\n",
    "                 validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = hist.history['loss']\n",
    "test_loss = hist.history['val_loss']\n",
    "training_acc = hist.history['acc']\n",
    "test_acc = hist.history['val_acc']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "\n",
    "plt.plot(epoch_count, training_acc, 'r--')\n",
    "plt.plot(epoch_count, test_acc, 'b-')\n",
    "plt.legend(['Training Acc', 'Validation Acc'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = preprocess.load_split_data(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, batch_size=1, verbose=1)\n",
    "Y_round = np.around(Y_pred)\n",
    "\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    preprocess.show_ndimg(X_test[i], Y_test[i])\n",
    "    preprocess.show_ndimg(Y_pred[i], Y_round[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "Y_pred = model.predict(X_train[0:n], batch_size=1, verbose=1)\n",
    "Y_round = np.around(Y_pred)\n",
    "\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    preprocess.show_ndimg(X_train[i], Y_train[i])\n",
    "    preprocess.show_ndimg(Y_pred[i], Y_round[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_total = np.prod(Y_train[0].shape)\n",
    "n_ones = np.sum(Y_train[0])\n",
    "n_zeros = n_total - n_ones\n",
    "print(n_total)\n",
    "print(n_ones)\n",
    "print(n_zeros)\n",
    "w_zero = n_ones / n_total\n",
    "w_ones = n_zeros / n_total\n",
    "print(w_zero)\n",
    "print(w_ones)\n",
    "print(w_zero + w_ones)\n",
    "print(1 - 0.0022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(Y_pred.shape[0]):\n",
    "    preprocess.show_ndimg(X_test[i], Y_test[i])\n",
    "    preprocess.show_ndimg(Y_pred[i], Y_round[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
