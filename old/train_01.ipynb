{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, merge, UpSampling2D, Cropping2D, ZeroPadding2D, Reshape, core, Convolution2D, Conv2DTranspose\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_00 import get_model\n",
    "import pickle\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "import imageio\n",
    "import augment\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = \"/home/matthias/projects/bio-unet/data/512/train\"\n",
    "d_test = \"/home/matthias/projects/bio-unet/data/512/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = utils.get_files(d_test, verbose=False)\n",
    "print(files[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = imageio.imread(\"/home/matthias/projects/bio-unet/data/512/test/11_x.png\")\n",
    "y = imageio.imread(\"/home/matthias/projects/bio-unet/data/512/test/11_y.png\")\n",
    "y = y.reshape((512, 512, 1))\n",
    "utils.show_img(x, y.reshape(y.shape[:-1]))\n",
    "for i in range(7):\n",
    "    print(i)\n",
    "    xn = augment_imgarr(x, i)\n",
    "    #yn = augment_imgarr(y, i)\n",
    "    utils.show_img(x, xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batches = 20\n",
    "dg = dataGenerator(files,batches=batches, normalize=True, augment=True)\n",
    "X, Y = next(dg)\n",
    "X, Y = next(dg)\n",
    "Xe, Ye = None, None\n",
    "for j in range(batches):\n",
    "    x = X[j][:]\n",
    "    y = Y[j][:]\n",
    "    y = y.reshape((512, 512))\n",
    "    z = y > 0\n",
    "    x[z] = 0\n",
    "    utils.show_img(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Y[0]\n",
    "x = X[0]\n",
    "z = y > 0\n",
    "z = z.reshape((512, 512))\n",
    "print(z.shape)\n",
    "print(x.shape)\n",
    "x[z] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(n_kernels=1, img_height=2084, img_width=2084)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.98, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "#K.get_session().run(tf.global_variables_initializer())\n",
    "#tensorboard = TensorBoard(log_dir=\"logs\") \n",
    "callbacks = [\n",
    "             EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           verbose=1),\n",
    "             ModelCheckpoint(\"model01.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)]\n",
    "train_generator = dataGenerator(files_train, batches=10, weights=True)\n",
    "valid_generator = dataGenerator(files_valid, batches=3, weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size determines the number of samples in each mini batch\n",
    "# steps_per_epoch the number of batch iterations before a training epoch is considered finished.\n",
    "# validation_steps\n",
    "hist = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=1,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=1,\n",
    "                    max_queue_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = hist.history['loss']\n",
    "test_loss = hist.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(splitdata.d_data, splitdata.f_test), \"rb\") as file:\n",
    "    files_test = pickle.load(file)\n",
    "if not files_test:\n",
    "    raise Exception(\"Could not load training files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = preprocess.load_data(files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, batch_size=len(X_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.around(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_pred.shape[0]):\n",
    "    preprocess.show_ndimg(Y_test[i], Y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_pred.shape[0]):\n",
    "    print(np.sum(Y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
