{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Input, merge, UpSampling2D, Cropping2D, ZeroPadding2D, Reshape, core, Convolution2D, Conv2DTranspose\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocess\n",
    "from model_00 import get_model\n",
    "import pickle\n",
    "import splitdata\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(splitdata.d_data, splitdata.f_train), \"rb\") as file:\n",
    "    files = pickle.load(file)\n",
    "if not files:\n",
    "    raise Exception(\"Could not load training files!\")\n",
    "print(\"Files: \", len(files))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(files)\n",
    "n_valid = int(np.ceil(len(files) * 0.15))\n",
    "files_train = files[n_valid:]\n",
    "files_valid = files[:n_valid]\n",
    "print(\"Files train: \", len(files_train))\n",
    "print(\"Files valid: \", len(files_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_imgarr(x, i):\n",
    "    if i % 4 == 0:\n",
    "        return np.rot90(x, k=i)\n",
    "    if i == 4:\n",
    "        return np.flip(x, 0)\n",
    "    if i == 5:\n",
    "        return np.flip(x, 1)\n",
    "    else:\n",
    "        return np.transpose(x, axes=(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(y):\n",
    "    n_ones = np.sum(y)\n",
    "    n_total = y.shape[1]**2\n",
    "    w_ones = 1 - n_ones / n_total\n",
    "    w_zeros = 1 - w_ones\n",
    "    w = y * (w_ones - w_zeros) + w_zeros\n",
    "    return w\n",
    "\n",
    "def dataGenerator(files, batches=1, weights=False):\n",
    "    files = itertools.cycle(files)\n",
    "    while True:\n",
    "        X = np.zeros((batches, 2084, 2084, 3), dtype=np.float32)\n",
    "        Y = np.zeros((batches, 2084, 2084, 1), dtype=np.float32)\n",
    "        if weights:\n",
    "            W = np.zeros((batches, 2084, 2084, 1), dtype=np.float32)\n",
    "        for i in range(batches):\n",
    "            f_png, f_csv, _ = next(files)\n",
    "            x = np.array(load_img(f_png))\n",
    "            y = preprocess.get_ndarray_from_csv(f_csv, X[i].shape[0], X[i].shape[1])\n",
    "            x = x / 255\n",
    "            idx = np.random.randint(0, 7)\n",
    "            X[i] = augment_imgarr(x, idx)\n",
    "            y = augment_imgarr(y, idx)\n",
    "            Y[i] = y\n",
    "            if weights:\n",
    "                W[i] = calc_weights(y)\n",
    "        if weights:\n",
    "            yield X, W\n",
    "        else:\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dataGenerator(files_train, batches=20, weights=True)\n",
    "for i in range(2):\n",
    "    X, Y = next(dg)\n",
    "    print(X.shape, Y.shape)\n",
    "    preprocess.show_ndimg(X[0], Y[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n_kernels=1, img_height=2084, img_width=2084)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.98, nesterov=True),\n",
    "              metrics=['accuracy'])\n",
    "#K.get_session().run(tf.global_variables_initializer())\n",
    "#tensorboard = TensorBoard(log_dir=\"logs\") \n",
    "callbacks = [\n",
    "             EarlyStopping(monitor='val_loss',\n",
    "                           patience=2,\n",
    "                           verbose=1),\n",
    "             ModelCheckpoint(\"model01.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)]\n",
    "train_generator = dataGenerator(files_train, batches=10, weights=True)\n",
    "valid_generator = dataGenerator(files_valid, batches=3, weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size determines the number of samples in each mini batch\n",
    "# steps_per_epoch the number of batch iterations before a training epoch is considered finished.\n",
    "# validation_steps\n",
    "hist = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=1,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=1,\n",
    "                    max_queue_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = hist.history['loss']\n",
    "test_loss = hist.history['val_loss']\n",
    "\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(splitdata.d_data, splitdata.f_test), \"rb\") as file:\n",
    "    files_test = pickle.load(file)\n",
    "if not files_test:\n",
    "    raise Exception(\"Could not load training files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = preprocess.load_data(files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, batch_size=len(X_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.around(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_pred.shape[0]):\n",
    "    preprocess.show_ndimg(Y_test[i], Y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(Y_pred.shape[0]):\n",
    "    print(np.sum(Y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
